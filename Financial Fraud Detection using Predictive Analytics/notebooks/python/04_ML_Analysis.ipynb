{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Analysis - Fraud Detection\n",
        "\n",
        "This notebook implements machine learning models for fraud detection:\n",
        "1. Data preprocessing and feature engineering\n",
        "2. Model training (Logistic Regression, Random Forest, XGBoost, LightGBM)\n",
        "3. Model evaluation and validation\n",
        "4. Feature importance analysis\n",
        "5. Model interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_path = Path('../../data/fraud_data.csv')\n",
        "df = pd.read_csv(data_path)\n",
        "print(f\"Data loaded: {df.shape}\")\n",
        "print(f\"Fraud rate: {df['isFraud'].mean():.4f}\")\n",
        "print(f\"Target distribution:\\n{df['isFraud'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "# Use key numerical features and handle missing values\n",
        "key_features = ['TransactionAmt', 'card1', 'card2', 'card3', 'card5', \n",
        "                'addr1', 'addr2', 'dist1', 'dist2']\n",
        "key_features = [f for f in key_features if f in df.columns]\n",
        "\n",
        "# Add some C and D features if available\n",
        "c_features = [col for col in df.columns if col.startswith('C') and col[1:].isdigit()][:10]\n",
        "d_features = [col for col in df.columns if col.startswith('D') and col[1:].isdigit()][:10]\n",
        "\n",
        "features = key_features + c_features + d_features\n",
        "features = [f for f in features if f in df.columns]\n",
        "\n",
        "print(f\"Selected {len(features)} features for modeling\")\n",
        "\n",
        "# Prepare data\n",
        "X = df[features].copy()\n",
        "y = df['isFraud'].copy()\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# Handle infinite values\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape}, Fraud rate: {y_train.mean():.4f}\")\n",
        "print(f\"Test set: {X_test.shape}, Fraud rate: {y_test.mean():.4f}\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Logistic Regression trained!\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, lr_pred_proba):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, lr_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Random Forest trained!\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, rf_pred_proba):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, rf_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 3: XGBoost\n",
        "print(\"Training XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
        "    eval_metric='auc'\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"XGBoost trained!\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, xgb_pred_proba):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, xgb_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 4: LightGBM\n",
        "print(\"Training LightGBM...\")\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_pred = lgb_model.predict(X_test)\n",
        "lgb_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"LightGBM trained!\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, lgb_pred_proba):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, lgb_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "models = {\n",
        "    'Logistic Regression': (lr_pred_proba, lr_pred),\n",
        "    'Random Forest': (rf_pred_proba, rf_pred),\n",
        "    'XGBoost': (xgb_pred_proba, xgb_pred),\n",
        "    'LightGBM': (lgb_pred_proba, lgb_pred)\n",
        "}\n",
        "\n",
        "# ROC Curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "for name, (pred_proba, pred) in models.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, pred_proba)\n",
        "    auc = roc_auc_score(y_test, pred_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../../outputs/figures/roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Model comparison table\n",
        "comparison = []\n",
        "for name, (pred_proba, pred) in models.items():\n",
        "    auc = roc_auc_score(y_test, pred_proba)\n",
        "    comparison.append({\n",
        "        'Model': name,\n",
        "        'AUC-ROC': auc,\n",
        "        'Accuracy': (pred == y_test).mean()\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison)\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\"*80)\n",
        "display(comparison_df.sort_values('AUC-ROC', ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (using Random Forest and XGBoost)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Random Forest feature importance\n",
        "rf_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "axes[0].barh(range(len(rf_importance)), rf_importance['importance'])\n",
        "axes[0].set_yticks(range(len(rf_importance)))\n",
        "axes[0].set_yticklabels(rf_importance['feature'])\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_title('Random Forest - Top 20 Feature Importance')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# XGBoost feature importance\n",
        "xgb_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "axes[1].barh(range(len(xgb_importance)), xgb_importance['importance'])\n",
        "axes[1].set_yticks(range(len(xgb_importance)))\n",
        "axes[1].set_yticklabels(xgb_importance['feature'])\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].set_title('XGBoost - Top 20 Feature Importance')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../../outputs/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 10 Features (Random Forest):\")\n",
        "display(rf_importance.head(10))\n",
        "print(\"\\nTop 10 Features (XGBoost):\")\n",
        "display(xgb_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save best model\n",
        "best_model = xgb_model  # XGBoost typically performs best\n",
        "joblib.dump(best_model, '../../outputs/models/best_model.pkl')\n",
        "joblib.dump(scaler, '../../outputs/models/scaler.pkl')\n",
        "print(\"Best model saved to outputs/models/best_model.pkl\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MACHINE LEARNING ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nBest Model: XGBoost\")\n",
        "print(f\"Best AUC-ROC: {roc_auc_score(y_test, xgb_pred_proba):.4f}\")\n",
        "print(f\"\\nModels trained: {len(models)}\")\n",
        "print(f\"Features used: {len(features)}\")\n",
        "print(\"\\nAnalysis complete!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
